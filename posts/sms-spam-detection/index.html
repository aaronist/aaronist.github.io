<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="description" content="We&amp;rsquo;ve all received those messages.
WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.
500 New Mobiles from 2021, MUST GO! Txt: NOKIA to No: 89545 &amp;amp; collect yours today!From ONLY £1 2optout 087187262701.50gbp/mtmsg18 TXTAUCTION
These intentionally misleading messages, commonly referred to as &amp;ldquo;spam&amp;rdquo;, are much more prolific in today&amp;rsquo;s world given the universality and convenience of SMS." />
<meta name="keywords" content=", machine learning, hmm, spam detection, algorithms" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://aaronist.github.io/posts/sms-spam-detection/" />


    <title>
        
            Discrete Hidden Markov Model for SMS Spam Detection :: Aaron Blume&#39;s Portfolio 
        
    </title>





<link rel="stylesheet" href="/main.949191c1dcc9c4a887997048b240354e47152016d821198f89448496ba42e491.css" integrity="sha256-lJGRwdzJxKiHmXBIskA1TkcVIBbYIRmPiUSElrpC5JE=">



    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="/favicon.ico">
    <meta name="msapplication-TileColor" content="">



  <meta itemprop="name" content="Discrete Hidden Markov Model for SMS Spam Detection">
  <meta itemprop="description" content="We’ve all received those messages.
WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.
500 New Mobiles from 2021, MUST GO! Txt: NOKIA to No: 89545 &amp; collect yours today!From ONLY £1 2optout 087187262701.50gbp/mtmsg18 TXTAUCTION
These intentionally misleading messages, commonly referred to as “spam”, are much more prolific in today’s world given the universality and convenience of SMS.">
  <meta itemprop="datePublished" content="2024-06-06T14:57:58-07:00">
  <meta itemprop="dateModified" content="2024-06-06T14:57:58-07:00">
  <meta itemprop="wordCount" content="1972">
  <meta itemprop="keywords" content="Machine Learning,Hmm,Spam Detection,Algorithms">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Discrete Hidden Markov Model for SMS Spam Detection">
  <meta name="twitter:description" content="We’ve all received those messages.
WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.
500 New Mobiles from 2021, MUST GO! Txt: NOKIA to No: 89545 &amp; collect yours today!From ONLY £1 2optout 087187262701.50gbp/mtmsg18 TXTAUCTION
These intentionally misleading messages, commonly referred to as “spam”, are much more prolific in today’s world given the universality and convenience of SMS.">







    <meta property="article:published_time" content="2024-06-06 14:57:58 -0700 PDT" />












        
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)']]                  
    }
  };
</script>
        
    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text ">
                hello</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="/posts">Posts</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            
        </span>
    </span>
</header>


            <div class="content">
                
  <main class="post">

    <div class="post-info">
      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock">
          <circle cx="12" cy="12" r="10"></circle>
          <polyline points="12 6 12 12 16 14"></polyline>
        </svg>
        10 minutes

        
      </p>
    </div>

    <article>
      <h1 class="post-title">
        <a href="https://aaronist.github.io/posts/sms-spam-detection/">Discrete Hidden Markov Model for SMS Spam Detection</a>
      </h1>

      

      

      

      <div class="post-content">
        <p>We&rsquo;ve all received those messages.</p>
<blockquote>
<p><em>WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.</em></p>
</blockquote>
<blockquote>
<p><em>500 New Mobiles from 2021, MUST GO! Txt: NOKIA to No: 89545 &amp; collect yours today!From ONLY £1 2optout 087187262701.50gbp/mtmsg18 TXTAUCTION</em></p>
</blockquote>
<p>These intentionally misleading messages, commonly referred to as &ldquo;spam&rdquo;, are much more prolific in today&rsquo;s world given the universality and convenience of SMS. Luckily for us, machine learning is also rapidly growing, and it can serve as a great mechanism for automatically filtering spam messages. In fact, there has already been extensive research done on this topic, and I would highly recommend taking a look at the machine learning models that others have deployed in order to tackle this problem. For example, check out the <a href="(https://arxiv.org/abs/2202.03480)">transfer learning technique used by Vijay Tida and Sonya Hsu</a> as well as the <a href="https://arxiv.org/abs/2304.01238">LLM used by Maxime Labonne and Sean Moran</a>.</p>
<h2 id="naïve-bayes">Naïve Bayes</h2>
<p>Bayesian models are a popular way of representing spam classification tasks. We can declare a binary indicator \(Y\) which identifies whether or not a message is spam and a variable \(x_i\) which represents the \(i\)-th word in a message. We can construct a naïve Bayes model by using the bag-of-words model and computing the probabilities of observing a word conditioned on the message&rsquo;s classification. For two arbitrary words, an example distribution might be something like:</p>
\[
\begin{aligned}
\hat{p}(\text{winner} | Y = 1) = 0.00923 \\
\hat{p}(\text{winner} | Y = 0) = 0.00039
\end{aligned}
\]
\[
\begin{aligned}
\hat{p}(\text{sometimes} | Y = 1) = 0.00007 \\
\hat{p}(\text{sometimes} | Y = 0) = 0.00298
\end{aligned}
\]
<p>The probability of observing a spammy word like &ldquo;winner&rdquo; is much more likely to show up in spam messages than ham (i.e. non-spam) messages. On the other hand, non-spammy words like &ldquo;sometimes&rdquo; are rarely going to be observed in spam messages and are more likely to appear in ham messages. Using this, we can compute a word&rsquo;s score as the log-odds ratio of its probabilities</p>
\[
\eta_i = \log\frac{p(x_i | Y = 1)}{p(x_i | Y = 0)}
\]
<p>and sum together the scores of each individual word in the message. The cumulative score of a message classifies it as spam or ham based on some threshold value (typically 0 with positive scores indicating spam and negative scores indicating ham).</p>
<p>Of course, you may have noticed that this is a very simple model and forces us to assume that each feature is conditionally independent given \(Y\). Each word is treated as an independent observation given the classification of the message being spam or ham. By then applying Bayes&rsquo; rule, we simply accumulate a preference for either category. More generally, we&rsquo;re marginalizing over \(Y\) by adding together the probabilities of what happens when \(Y\) is true (i.e. the product of \(Y = 1\) multiplied against the product of the individual conditional distributions) and when \(Y\) is false.</p>
\[
p(Y = 1 | x) = \frac{p(Y = 1) \prod_i p(x_i | Y = 1)}{p(Y = 1) \prod_i p(x_i | Y = 1) + p(Y = 0) \prod_i p(x_i | Y = 0)}
\]
<h2 id="hidden-markov-model">Hidden Markov Model</h2>
<p>The simplicity of the naïve Bayes model is nice, but we can take this a step further by introducing the Markov assumption: that the future state depends solely on the current state. One of the defining features of a hidden Markov model (HMM) is that it&rsquo;s a memoryless stochastic process. This means that we can easily compute future probabilities in a dynamic fashion without needing to retain all of the preceding states of the model.</p>
<p>Before continuing any further, I want to give credit to Tian Xia and Xuemin Chen for <a href="https://doi.org/10.3390/app10145011">their research in using HMMs for spam detection</a>. Much of what I completed is based on their research.</p>
<h3 id="preprocessing">Preprocessing</h3>
<p>Before we start training any sort of HMM, it&rsquo;s important to get our data in a proper format. For my data, I used the <a href="https://archive.ics.uci.edu/dataset/228/sms+spam+collection">SMS Spam Collection</a> dataset, collected by Tiago Almeida and Jos Hidalgo, which is hosted in the UCI Machine Learning Repository.</p>
<p>As with any proper machine learning model, we need to split our data into two categories: training and testing. Because the SMS Spam Collection dataset is not very large (with only 5,574 entries), I decided to use \(\frac{2}{3}\) of the data for training and \(\frac{1}{3}\) of the data for testing.</p>
<p>The next step for my preprocessing procedure is commonly used in information retrieval methods. While it would be nice to use term-ranking metrics like tf-idf, this is not practical for SMS messages due to the high variation of words used by individuals in their messages. For those unaware, tf-idf is a measure of a term&rsquo;s importance to a particular document/message within a provided corpus. For a term \(t\), a document \(d\), and a corpus of documents \(D\), the tf-idf score is calculated as follows:</p>
\[
\text{tf-idf}(t, d, D) = \frac{f_{t, d}}{\sum_{t' \in d} f_{t', d}} \cdot \log \left(\frac{|D|}{|\{d: d \in D\text{ and }t \in d\}|}\right)
\]
<p>The first term of the multiplication is the relative frequency of a term \(t\) in a document \(d\), where \(f_{t, d}\) is the number of times term \(t\) appears in document \(d\). The second term of the multiplication is the inverse document frequency of a term \(t\) in a corpus \(D\), i.e. the log of the number of documents in the corpus divided by the number of documents in the corpus that contain the term \(t\). The inverse document frequency essentially explains how rare a term is in the corpus; the rarer a term, the more important it is.</p>
<p>So, going back to the problem regarding SMS messages, you might be able to see that using the inverse document frequency has a glaring problem. In a corpus of SMS messages, many of the words we encounter will likely only appear once or twice since people tend to use different acronyms, different abbreviations, different spellings, and so on. As a result, we can&rsquo;t properly assign the importance of a word with this metric because <em>many</em> of the words will be considered important.</p>
<p>This means we won&rsquo;t be using tf-idf scores to weight each term in a message, but we can eliminate some of this inconsistency by tokenizing, lowercasing, and lemmatizing each message. Python&rsquo;s NLTK package allows us to easily tokenize and lemmatize the messages as well as remove stop words from the dataset.</p>
<h3 id="training">Training</h3>
<p>After obtaining the preprocessed data, the next step is to train the HMM. A basic HMM \(\lambda\) can be decomposed into 3 parameters: \(\lambda = (A, B, \pi)\). Matrix \(A\) represents the state transition matrix, matrix \(B\) represents the observation probability matrix, and \(\pi\) represents the initial state probability distribution. Since we need to initialize these parameters, it&rsquo;s easiest to assign equal probabilities to each state (spam and ham) for \(A\) and \(\pi\). \(B\) is computed by computing the disjoint relative frequencies of each word in the spam and ham training datasets.</p>
<p>Training the HMM is the ultimate crux of this problem, but thankfully, the Baum-Welch algorithm is a popular way to train HMMs with these two high-level steps:</p>
<ol>
<li>Expectation step: Use observation sequence and current \(\lambda\) to compute the log-likelihood of the model.</li>
<li>Maximization step: Recompute the model&rsquo;s parameters to maximize the log-likelihood.</li>
</ol>
<p>The entire premise is that we want to find the HMM parameters such that a given observation sequence has the highest likelihood of occurring. The Baum-Welch algorithm achieves this by looping those two steps until the parameters converge (or until we&rsquo;re satisfied with the results).</p>
<p>Therefore, we can flatten the preprocessed training messages to treat it as a single observation sequence and use that to train the HMM.</p>
<h3 id="predicting">Predicting</h3>
<p>We&rsquo;re in the final stage of this problem, and that&rsquo;s determining how we should predict the classification of an SMS message. Since each word is treated as a feature in the HMM, we&rsquo;re going to end up with a classification for every word in athe message. However, we want a single classification for the entire message, so it&rsquo;s easiest to just select the majority vote as the classification for the message.</p>
<p>The Viterbi algorithm is a dynamic programming decoding algorithm which determines the most probable path (i.e. sequence of hidden states) which should result in the given observed sequence. For example, given the observed sequence</p>
\[
\text{hello Friend! I have a great offer 4 u. call me now at...}
\]
<p>the Viterbi algorithm will obtain the sequence of hidden states that makes this observation most likely. In this case, it might be something like</p>
\[
\underbrace{\text{hello}}_{\text{ham}}\text{ }\underbrace{\text{Friend!}}_{\text{spam}}\text{ }\underbrace{\text{I}}_{\text{ham}}\text{ }\underbrace{\text{have}}_{\text{ham}}\text{ }\underbrace{\text{a}}_{\text{ham}}\text{ }\underbrace{\text{great}}_{\text{ham}}\text{ }\underbrace{\text{offer}}_{\text{spam}}\text{ }\underbrace{\text{4}}_{\text{spam}}\text{ }\underbrace{\text{u.}}_{\text{spam}}\text{ }\underbrace{\text{call}}_{\text{spam}}\text{ }\underbrace{\text{me}}_{\text{spam}}\text{ }\underbrace{\text{now}}_{\text{spam}}\text{ }\underbrace{\text{at}}_{\text{spam}}\text{...}
\]
<p>The message starts off seeming like ham, but after a few suspicous words, the Viterbi algorithm realizes that the most probable path is along the spam route. (Note that this example is not entirely accurate as I&rsquo;m including stop words and ignoring fluctuations in the algorithm, but the principle is the same.)</p>
<p>Similar to the Baum-Welch algorithm, the Viterbi algorithm has a forward-backward structure in which it firsts iterates forward through the sequence to compute the probabilities of observing that sequence, and then it iterates backwards through those probabilities to determine which assignments correspond to the maximum likelihood. This task is equivalent to solving:</p>
\[
\underset{Y_0, \ Y_1, \ ..., \ Y_N}{\text{arg max}} \ p(Y_0, \ Y_1, \ ..., \ Y_N, \ X_0 = x_0, \ X_1 = x_1, \ ..., \ X_N = x_N)
\]
<p>That is, in a message of length \(N\), we&rsquo;re computing the \(\text{arg max}\) of hidden states across the entire observation sequence. Thus, by taking each message in the testing dataset and preprocessing it the same way as the training dataset, we can feed each message into the Viterbi algorithm, find the majority vote classification, and obtain the model&rsquo;s prediction for that message&rsquo;s classification.</p>
<h3 id="the-outcome">The Outcome</h3>
<p>In the end, I was able to obtain a total accuracy of around 90% with this model. Ham messages tended to be correctly classified 90-95% of the time, and spam messages were classified correctly roughly 75-85% of the time.</p>
<p>There does appear to be a high false negative rate with this model, but practically speaking, I think this is preferred to having a high false positive rate since filtering out potentially important messages is more inconvenient than not filtering out spam messages. Of course, the parameters can easily be tuned to prefer one type over the other. The model I designed defaults to predicting ham for words it has never encountered before, and this is largely the cause of the high false negative rate. If I change this behavior to predict spam for such words, I end up with a much higher spam detection rate (&gt; 95%) but a lower ham detection rate (&lt; 75%).</p>
<p>While these results are nice, there is definitely room for improvement. For instance, it might be viable to use enhanced language preprocessing steps to better tokenize and simplify the observation state distribution. It would also certainly be useful to obtain a larger dataset for more training data.</p>
<p>The big takeaway, I feel, is that simple ML models definitely have the capacity to be nearly as effective as complex models. All 1,858 SMS messages in the testing dataset were classified in just a few seconds, and I think that&rsquo;s one of the major advantages of using a simple model with simple algorithms.</p>
<h2 id="references">References</h2>
<p>[1] D. Batista. &ldquo;<a href="https://www.davidsbatista.net/blog/2017/11/11/HHM_and_Naive_Bayes/">Hidden Markov Model and Naive Bayes relationship</a>&rdquo;. 2017.</p>
<p>[2] M. Labonne, S. Moran. &ldquo;<a href="https://arxiv.org/abs/2304.01238">Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection</a>&rdquo;. 2023.</p>
<p>[3] T. Almeida, J. Hidalgo. &ldquo;<a href="https://doi.org/10.24432/C5CC84">SMS Spam Collection</a>&rdquo;. <em>UCI Machine Learning Repository</em>. 2012.</p>
<p>[4] T. Xia, X. Chen. &ldquo;<a href="https://doi.org/10.3390/app10145011">A Discrete Hidden Markov Model for SMS Spam Detection</a>&rdquo;. <em>Appl. Sci.</em> 2020.</p>
<p>[5] V. Tida, S. Hsu. &ldquo;<a href="https://arxiv.org/abs/2202.03480">Universal Spam Detection using Transfer Learning of BERT Model</a>&rdquo;. 2022.</p>

      </div>
    </article>

    <hr />

    <div class="post-info">
      
    <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg>

        <span class="tag"><a href="https://aaronist.github.io/tags/machine-learning/">machine learning</a></span>
        <span class="tag"><a href="https://aaronist.github.io/tags/hmm/">hmm</a></span>
        <span class="tag"><a href="https://aaronist.github.io/tags/spam-detection/">spam detection</a></span>
        <span class="tag"><a href="https://aaronist.github.io/tags/algorithms/">algorithms</a></span>
        
    </p>

      

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text">
          <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
          <line x1="16" y1="13" x2="8" y2="13"></line>
          <line x1="16" y1="17" x2="8" y2="17"></line>
          <polyline points="10 9 9 9 8 9"></polyline>
        </svg>
        1972 Words
      </p>

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
          <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="16" y1="2" x2="16" y2="6"></line>
          <line x1="8" y1="2" x2="8" y2="6"></line>
          <line x1="3" y1="10" x2="21" y2="10"></line>
        </svg>
        
          2024-06-06 14:57
        

         
          
        
      </p>
    </div>

    

    

    

    

  </main>

            </div>

            
                <footer class="footer">
    
    <div class="footer__inner">
        <div class="footer__content">
            
            
            <span>&copy 2024 <a href="https://aaronist.github.io/">Aaron Blume</a> | Powered by <a href="https://gohugo.io/">Hugo</a></span>
            
            
        </div>
    </div>
    
    
</footer>

            
        </div>

        



<script type="text/javascript" src="/bundle.min.85fad2de4f13fec3bcb3b3cb10430cdb44a7b4a9749b32938241a5c6e77718df7624f1002b880521fdc26e24ec1077fda214bf1cb36ee3045510760d09638cae.js" integrity="sha512-hfrS3k8T/sO8s7PLEEMM20SntKl0mzKTgkGlxud3GN92JPEAK4gFIf3CbiTsEHf9ohS/HLNu4wRVEHYNCWOMrg=="></script>




    </body>
</html>
