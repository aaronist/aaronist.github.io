<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Aaron Blume&#39;s Portfolio</title>
        <link>https://aaronist.github.io/posts/</link>
        <description>Recent content in Posts on Aaron Blume&#39;s Portfolio</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>&amp;copy 2024 &lt;a href=&#34;https://aaronist.github.io/&#34;&gt;Aaron Blume&lt;/a&gt; | Powered by &lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt;</copyright>
        <lastBuildDate>Thu, 06 Jun 2024 14:57:58 -0700</lastBuildDate>
        <atom:link href="https://aaronist.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Discrete Hidden Markov Model for SMS Spam Detection</title>
            <link>https://aaronist.github.io/posts/sms-spam-detection/</link>
            <pubDate>Thu, 06 Jun 2024 14:57:58 -0700</pubDate>
            
            <guid>https://aaronist.github.io/posts/sms-spam-detection/</guid>
            <description>We&amp;rsquo;ve all received those messages.
WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.
500 New Mobiles from 2021, MUST GO! Txt: NOKIA to No: 89545 &amp;amp; collect yours today!From ONLY £1 2optout 087187262701.50gbp/mtmsg18 TXTAUCTION
These intentionally misleading messages, commonly referred to as &amp;ldquo;spam&amp;rdquo;, are much more prolific in today&amp;rsquo;s world given the universality and convenience of SMS.</description>
            <content type="html"><![CDATA[<p>We&rsquo;ve all received those messages.</p>
<blockquote>
<p><em>WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.</em></p>
</blockquote>
<blockquote>
<p><em>500 New Mobiles from 2021, MUST GO! Txt: NOKIA to No: 89545 &amp; collect yours today!From ONLY £1 2optout 087187262701.50gbp/mtmsg18 TXTAUCTION</em></p>
</blockquote>
<p>These intentionally misleading messages, commonly referred to as &ldquo;spam&rdquo;, are much more prolific in today&rsquo;s world given the universality and convenience of SMS. Luckily for us, machine learning is also rapidly growing, and it can serve as a great mechanism for automatically filtering spam messages. In fact, there has already been extensive research done on this topic, and I would highly recommend taking a look at the machine learning models that others have deployed in order to tackle this problem. For example, check out the <a href="(https://arxiv.org/abs/2202.03480)">transfer learning technique used by Vijay Tida and Sonya Hsu</a> as well as the <a href="https://arxiv.org/abs/2304.01238">LLM used by Maxime Labonne and Sean Moran</a>.</p>
<h2 id="naïve-bayes">Naïve Bayes</h2>
<p>Bayesian models are a popular way of representing spam classification tasks. We can declare a binary indicator \(Y\) which identifies whether or not a message is spam and a variable \(x_i\) which represents the \(i\)-th word in a message. We can construct a naïve Bayes model by using the bag-of-words model and computing the probabilities of observing a word conditioned on the message&rsquo;s classification. For two arbitrary words, an example distribution might be something like:</p>
\[
\begin{aligned}
\hat{p}(\text{winner} | Y = 1) = 0.00923 \\
\hat{p}(\text{winner} | Y = 0) = 0.00039
\end{aligned}
\]
\[
\begin{aligned}
\hat{p}(\text{sometimes} | Y = 1) = 0.00007 \\
\hat{p}(\text{sometimes} | Y = 0) = 0.00298
\end{aligned}
\]
<p>The probability of observing a spammy word like &ldquo;winner&rdquo; is much more likely to show up in spam messages than ham (i.e. non-spam) messages. On the other hand, non-spammy words like &ldquo;sometimes&rdquo; are rarely going to be observed in spam messages and are more likely to appear in ham messages. Using this, we can compute a word&rsquo;s score as the log-odds ratio of its probabilities</p>
\[
\begin{aligned}
\eta_i = \log\frac{p(x_i | Y = 1)}{p(x_i | Y = 0)}
\end{aligned}
\]
<p>and sum together the scores of each individual word in the message. The cumulative score of a message classifies it as spam or ham based on some threshold value (typically 0 with positive scores indicating spam and negative scores indicating ham).</p>
<p>Of course, you may have noticed that this is a very simple model and forces us to assume that each feature is conditionally independent given \(Y\). Each word is treated as an independent observation given the classification of the message being spam or ham. By then applying Bayes&rsquo; rule, we simply accumulate a preference for either category. More generally, we&rsquo;re marginalizing over \(Y\) by adding together the probabilities of what happens when \(Y\) is true (i.e. the product of \(Y = 1\) multiplied against the product of the individual conditional distributions) and when \(Y\) is false.</p>
\[
\begin{aligned}
p(Y = 1 | x) = \frac{p(Y = 1) \prod_i p(x_i | Y = 1)}{p(Y = 1) \prod_i p(x_i | Y = 1) + p(Y = 0) \prod_i p(x_i | Y = 0)}
\end{aligned}
\]
<h2 id="hidden-markov-model">Hidden Markov Model</h2>
<p>The simplicity of the naïve Bayes model is nice, but we can take this a step further by introducing the Markov assumption: that the future state depends solely on the current state. One of the defining features of a hidden Markov model (HMM) is that it&rsquo;s a memoryless stochastic process. This means that we can easily compute future probabilities in a dynamic fashion without needing to retain all of the preceding states of the model.</p>
<p>Before continuing any further, I want to give credit to Tian Xia and Xuemin Chen for <a href="https://doi.org/10.3390/app10145011">their research in using HMMs for spam detection</a>. Much of what I completed is based on their research.</p>
<h3 id="preprocessing">Preprocessing</h3>
<p>Before we start training any sort of HMM, it&rsquo;s important to get our data in a proper format. For my data, I used the <a href="https://archive.ics.uci.edu/dataset/228/sms+spam+collection">SMS Spam Collection</a> dataset, collected by Tiago Almeida and Jos Hidalgo, which is hosted in the UCI Machine Learning Repository.</p>
<h2 id="references">References</h2>
<p>[1] M. Labonne, S. Moran. &ldquo;<a href="https://arxiv.org/abs/2304.01238">Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection</a>&rdquo;. 2023.</p>
<p>[2] T. Almeida, J. Hidalgo. &ldquo;<a href="https://doi.org/10.24432/C5CC84">SMS Spam Collection</a>&rdquo;. <em>UCI Machine Learning Repository</em>. 2012.</p>
<p>[3] T. Xia, X. Chen. &ldquo;<a href="https://doi.org/10.3390/app10145011">A Discrete Hidden Markov Model for SMS Spam Detection</a>&rdquo;. <em>Appl. Sci.</em> 2020.</p>
<p>[4] V. Tida, S. Hsu. &ldquo;<a href="https://arxiv.org/abs/2202.03480">Universal Spam Detection using Transfer Learning of BERT Model</a>&rdquo;. 2022.</p>
]]></content>
        </item>
        
    </channel>
</rss>
